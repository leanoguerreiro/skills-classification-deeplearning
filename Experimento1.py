import subprocess
import time
import sys
import os
import requests

TELEGRAM_BOT_TOKEN = os.environ['TELEGRAM_BOT_TOKEN']
TELEGRAM_CHAT_ID = os.environ['TELEGRAM_CHAT_ID']

def send_telegram_message(message):
    """Envia uma mensagem para o Telegram."""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        'chat_id': TELEGRAM_CHAT_ID,
        'text': message,
        'parse_mode': 'HTML' # Permite formata√ß√£o em HTML (negrito, it√°lico, etc.)
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status() # Lan√ßa uma exce√ß√£o para c√≥digos de status HTTP de erro
        # print(f"Notifica√ß√£o Telegram enviada: {message}") # Opcional: para depura√ß√£o
    except requests.exceptions.RequestException as e:
        print(f"Erro ao enviar notifica√ß√£o Telegram: {e}")

# Caminho para o seu script principal (main.py)
main_script_path = 'main.py'  # Assumindo que main.py est√° na mesma pasta

# Lista de dicion√°rios, onde cada dicion√°rio representa um experimento
# e cont√©m o nome do modelo e o batch_size a ser usado.
# Adicione os novos par√¢metros 'patience' e 'save_best' conforme sua necessidade.
experimentos = [
    # Experimento 1: Sem ru√≠do
    # ConvNeXt Base
    # Conhecido por ser robusto. O LR padr√£o (0.0001) √© um bom in√≠cio.
    # IMG_SIZE 224x224 √© o padr√£o para a maioria dos modelos pr√©-treinados em ImageNet.
    {'model_name': 'convnext_base', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 20, 'patience': 10,
     'save_best': True, 'no_data_augmentation': False, # Manter aumento de dados ativo
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # EfficientFormer L3
    # Modelos EfficientFormer s√£o projetados para efici√™ncia. O LR padr√£o deve ser OK.
    # O tamanho da imagem de 224x224 √© comum.
    {'model_name': 'efficientformer_l3', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 20, 'patience': 10,
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # RegNetY_040
    # Arquitetura tipo CNN. LR 0.0001 √© um bom ponto de partida.
    # Tamanho 224x224 √© padr√£o.
    {'model_name': 'regnety_040', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 20, 'patience': 10,
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # MobileViTv2_200
    # Desenvolvido para dispositivos m√≥veis, combina ViT com MobileNet.
    # LR padr√£o √© um bom come√ßo. 224x224 √© o tamanho mais comum.
    {'model_name': 'mobilevitv2_200', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 20, 'patience': 10,
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # ViT Small Patch16 224
    # Vision Transformers podem ser um pouco mais sens√≠veis ao LR.
    # 0.0001 geralmente funciona bem para fine-tuning. 224x224 √© o tamanho de pr√©-treinamento.
    {'model_name': 'vit_small_patch16_224', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 25, 'patience': 12, # Aumentar √©pocas e paci√™ncia
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # Swin S3 Small 224
    # Swin Transformers s√£o poderosos. LR 0.0001 √© adequado.
    # 224x224 √© o tamanho de pr√©-treinamento mais comum.
    {'model_name': 'swin_s3_small_224', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 25, 'patience': 12, # Aumentar √©pocas e paci√™ncia
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # DenseNetBlur121d
    # Varia√ß√£o da DenseNet. Um LR de 0.0001 √© um bom come√ßo.
    # 224x224 √© o tamanho padr√£o.
    {'model_name': 'densenetblur121d', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 20, 'patience': 10,
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # DeiT3 Small Patch16 224
    # Outro modelo baseado em Transformer, semelhante ao ViT.
    # LR 0.0001 deve ser bom. 224x224 √© o tamanho de pr√©-treinamento.
    {'model_name': 'deit3_small_patch16_224', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 25, 'patience': 12, # Aumentar √©pocas e paci√™ncia
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # VGG19_bn
    # Modelos VGG s√£o mais antigos, mas ainda robustos.
    # Um LR de 0.0001 geralmente funciona. 224x224 √© o padr√£o.
    {'model_name': 'vgg19_bn', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 20, 'patience': 10,
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)},

    # Vitamin Small 224
    # Mais um modelo Transformer.
    # Mantenha o LR 0.0001 e 224x224.
    {'model_name': 'vitamin_small_224', 'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 25, 'patience': 12, # Aumentar √©pocas e paci√™ncia
     'save_best': True, 'no_data_augmentation': False,
     'apply_gaussian_noise_train': False, 'gaussian_noise_std_train': 0.0,
     'apply_gaussian_noise_val_test': False, 'test_noise_std_val': 0.0,
     'img_size': (224, 224)}
]

start_time_global = time.time()
send_telegram_message("ü§ñ Iniciando todos os experimentos...")

print("========================================")
print("INICIANDO TODOS OS EXPERIMENTOS.")
print("========================================")

# Itera sobre a lista de experimentos e executa cada um
all_experiments_successful = True # Nova flag para rastrear o sucesso geral
for i, experimento in enumerate(experimentos):
    model_name = experimento['model_name']
    batch_size = experimento['batch_size']
    learning_rate = experimento['learning_rate']
    patience = experimento.get('patience', 5)
    save_best = experimento.get('save_best', True)
    disable_data_augmentation_flag = experimento.get('no_data_augmentation', False)
    img_size = experimento.get('img_size', (224, 224))
    epochs = experimento.get('epochs', 25)

    apply_gaussian_noise_train = experimento.get('apply_gaussian_noise_train', False)
    gaussian_noise_std_train = experimento.get('gaussian_noise_std_train', 0.0)
    apply_gaussian_noise_val_test = experimento.get('apply_gaussian_noise_val_test', False)
    test_noise_std_val = experimento.get('test_noise_std_val', 0.0)

    command = [
        sys.executable,
        main_script_path,
        '--model_name', model_name,
        '--batch_size', str(batch_size),
        '--learning_rate', str(learning_rate),
        '--patience', str(patience),
        '--img_size', str(img_size[0]),str(img_size[1]),
        '--epochs', str(epochs),
    ]

    if save_best:
        command.append('--save_best')

    if disable_data_augmentation_flag:
        command.append('--no_data_augmentation')

    if apply_gaussian_noise_train:
        command.append('--apply_gaussian_noise_train')
        if gaussian_noise_std_train > 0:
            command.extend(['--gaussian_noise_std', str(gaussian_noise_std_train)])

    if apply_gaussian_noise_val_test:
        command.append('--apply_gaussian_noise_val_test')
        if test_noise_std_val > 0:
            command.extend(['--test_noise_std', str(test_noise_std_val)])

    print(f"\n========================================")
    print(f"EXECUTANDO EXPERIMENTO {i + 1}/{len(experimentos)}:")
    print(f"  Modelo: {model_name}")
    print(f"  Batch Size: {batch_size}")
    print(f"  LR: {learning_rate}")
    print(f"  Epochs: {epochs}")
    print(f"  Paci√™ncia ES: {patience}")
    print(f"  Salvar Melhor: {save_best}")
    print(f"  Aumento de Dados Desabilitado: {disable_data_augmentation_flag}")
    print(f"  Ru√≠do Treino: {'Sim' if apply_gaussian_noise_train else 'N√£o'} (std={gaussian_noise_std_train})")
    print(f"  Ru√≠do Valida√ß√£o (Teste Robustez): {'Sim' if apply_gaussian_noise_val_test else 'N√£o'} (std={test_noise_std_val})")
    print(f"Comando: {' '.join(command)}")
    print(f"========================================")

    send_telegram_message(
        f"üöÄ **Iniciando Experimento {i + 1}/{len(experimentos)}:**\n"
        f"Modelo: `{model_name}`\n"
        f"Batch Size: `{batch_size}`\n"
        f"LR: `{learning_rate}`\n"
        f"Epochs: `{epochs}`\n"
        f"Aumento Dados: `{'N√£o' if disable_data_augmentation_flag else 'Sim'}`\n"
        f"Ru√≠do Treino: `{'Sim' if apply_gaussian_noise_train else 'N√£o'}` (std={gaussian_noise_std_train})\n"
        f"Ru√≠do Val: `{'Sim' if apply_gaussian_noise_val_test else 'N√£o'}` (std={test_noise_std_val})"
    )

    start_time_experiment = time.time()
    current_experiment_successful = False # Flag para o sucesso do experimento atual

    try:
        subprocess.run(command, check=True)
        current_experiment_successful = True
        message_status = "‚úÖ **CONCLU√çDO COM SUCESSO!**"
        print(f"----------------------------------------")
        print(f"EXPERIMENTO COM {model_name} CONCLU√çDO COM SUCESSO.")
        print(f"----------------------------------------\n")

    except subprocess.CalledProcessError as e:
        message_status = f"‚ùå **FALHA NO EXPERIMENTO!** (Modelo: `{model_name}`, Erro: C√≥digo {e.returncode})"
        print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print(f"ERRO ao executar o experimento com {model_name}: {e}")
        print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
        all_experiments_successful = False # Marca que pelo menos um experimento falhou
    except FileNotFoundError:
        message_status = f"‚ö†Ô∏è **ERRO FATAL: main.py n√£o encontrado!** (Modelo: `{model_name}`)"
        print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print(f"ERRO: O arquivo {main_script_path} n√£o foi encontrado.")
        print(f"Verifique se 'main.py' est√° no caminho correto.")
        print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
        all_experiments_successful = False # Marca que pelo menos um experimento falhou

    # --- Este bloco SEMPRE ser√° executado, independentemente de sucesso ou falha ---
    end_time_experiment = time.time()
    duration_experiment = end_time_experiment - start_time_experiment
    duration_formatted = time.strftime("%Hh %Mm %Ss", time.gmtime(duration_experiment))

    send_telegram_message(
        f"üìä Experimento de `{model_name}` {message_status}\n"
        f"Dura√ß√£o: `{duration_formatted}`\n"
        f"Progresso Total: `{i + 1}/{len(experimentos)}`"
    )
    # --- Fim do bloco que SEMPRE ser√° executado ---

    # Se o experimento atual falhou e voc√™ quer PARAR TUDO, use este if
    if not current_experiment_successful and 'break' in locals(): # Verifica se a exce√ß√£o fez um 'break'
        print("Interrompendo a execu√ß√£o de experimentos restantes devido a uma falha.")
        break # Sai do loop for

    # Pausa APENAS se n√£o for o √öLTIMO experimento da lista
    if (i < len(experimentos) - 1) and current_experiment_successful:
        print("Pausa de 3 minutos para resfriamento do sistema e libera√ß√£o de VRAM...")
        send_telegram_message(f"üò¥ Pausa de 3 minutos para resfriamento...")
        time.sleep(180)

# --- Mensagem de conclus√£o global ---
end_time_global = time.time()
total_duration = end_time_global - start_time_global
total_duration_formatted = time.strftime("%Hh %Mm %Ss", time.gmtime(total_duration))

if all_experiments_successful:
    final_message = f"üèÅ **TODOS OS EXPERIMENTOS FORAM CONCLU√çDOS COM SUCESSO!**\n" \
                    f"Tempo Total: `{total_duration_formatted}`"
else:
    final_message = f"üö® **EXECU√á√ÉO DE EXPERIMENTOS FINALIZADA COM FALHAS!**\n" \
                    f"Verifique os logs acima para detalhes. Tempo Total: `{total_duration_formatted}`"

send_telegram_message(final_message)
print("========================================")
print("TODOS OS EXPERIMENTOS FORAM EXECUTADOS.")
print("========================================")
